{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this code twice after the stage0 and stage0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_log/aligntts/stage0/checkpoint_1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import hparams\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.model import Model\n",
    "from text import text_to_sequence, sequence_to_text\n",
    "from denoiser import Denoiser\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import librosa\n",
    "from modules.loss import MDNLoss\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "data_type = 'char'\n",
    "checkpoint_path = f\"training_log/aligntts/stage0/checkpoint_40000\"\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "checkpoint_path = sorted(glob(\"training_log/aligntts/stage0/checkpoint_*\"))[0]\n",
    "\n",
    "print(checkpoint_path)\n",
    "\n",
    "\n",
    "state_dict = {}\n",
    "for k, v in torch.load(checkpoint_path)['state_dict'].items():\n",
    "    state_dict[k[7:]]=v\n",
    "\n",
    "\n",
    "model = Model(hparams).cuda()\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.cuda().eval()\n",
    "criterion = MDNLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = ['train', 'val', 'test']\n",
    "batch_size=64\n",
    "batch_size = 16\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    with open(f'filelists/ljs_audio_text_{dataset}_filelist.txt', 'r') as f:\n",
    "        lines_raw = [line.split('|') for line in f.read().splitlines()]\n",
    "        lines_list = [ lines_raw[batch_size*i:batch_size*(i+1)] \n",
    "                      for i in range(len(lines_raw)//batch_size+1)]\n",
    "        \n",
    "    for batch in tqdm(lines_list):\n",
    "        \n",
    "        single_loop_start = time.perf_counter()\n",
    "        \n",
    "        file_list, text_list, mel_list = [], [], []\n",
    "        text_lengths, mel_lengths=[], []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            file_name, _, text = batch[i]\n",
    "            file_list.append(file_name)\n",
    "            seq_path = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                               f'{data_type}_seq')\n",
    "            mel_path = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                               'melspectrogram')\n",
    "            try:\n",
    "                seq = torch.from_numpy(np.load(f'{seq_path}/{file_name}_sequence.npy'))\n",
    "            except FileNotFoundError:\n",
    "                with open(f'{seq_path}/{file_name}_sequence.pkl', 'rb') as f:\n",
    "                    seq = pkl.load(f)\n",
    "            \n",
    "            try:\n",
    "                mel = torch.from_numpy(np.load(f'{mel_path}/{file_name}_melspectrogram.npy'))\n",
    "            except FileNotFoundError:\n",
    "                with open(f'{mel_path}/{file_name}_melspectrogram.pkl', 'rb') as f:\n",
    "                    mel = pkl.load(f)\n",
    "            \n",
    "            text_list.append(seq)\n",
    "            mel_list.append(mel)\n",
    "            text_lengths.append(seq.size(0))\n",
    "            mel_lengths.append(mel.size(1))\n",
    "            \n",
    "        io_time = time.perf_counter()\n",
    "            \n",
    "        text_lengths = torch.LongTensor(text_lengths)\n",
    "        mel_lengths = torch.LongTensor(mel_lengths)\n",
    "        text_padded = torch.zeros(len(batch), text_lengths.max().item(), dtype=torch.long)\n",
    "        mel_padded = torch.zeros(len(batch), hparams.n_mel_channels, mel_lengths.max().item())\n",
    "        \n",
    "        for j in range(len(batch)):\n",
    "            text_padded[j, :text_list[j].size(0)] = text_list[j]\n",
    "            mel_padded[j, :, :mel_list[j].size(1)] = mel_list[j]\n",
    "        \n",
    "        text_padded = text_padded.cuda()\n",
    "        mel_padded = mel_padded.cuda()\n",
    "        text_lengths = text_lengths.cuda()\n",
    "        mel_lengths = mel_lengths.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model_start = time.perf_counter()\n",
    "            \n",
    "            encoder_input = model.Prenet(text_padded)\n",
    "            hidden_states, _ = model.FFT_lower(encoder_input, text_lengths)\n",
    "            mu_sigma = model.get_mu_sigma(hidden_states)\n",
    "            _, log_prob_matrix = criterion(mu_sigma, mel_padded, text_lengths, mel_lengths)\n",
    "            \n",
    "            viterbi_start = time.perf_counter()\n",
    "\n",
    "            align = model.viterbi(log_prob_matrix, text_lengths, mel_lengths).to(torch.long)\n",
    "            alignments = list(torch.split(align,1))\n",
    "            \n",
    "            viterbi_end = time.perf_counter()\n",
    "        \n",
    "        print('VT Time: ', end=' ')\n",
    "        print(f'{viterbi_end - viterbi_start:.6f} / {viterbi_end - single_loop_start:.6f} = ' +\n",
    "             f'{(viterbi_end - viterbi_start) / (viterbi_end - single_loop_start) * 100:5.2f}%')\n",
    "        \n",
    "        print('IO Time: ', end=' ')\n",
    "        print(f'{io_time - single_loop_start:.6f} / {viterbi_end - single_loop_start:.6f} = ' +\n",
    "             f'{(io_time - single_loop_start) / (viterbi_end - single_loop_start) * 100:5.2f}%')\n",
    "        \n",
    "        print('DL Time: ', end=' ')\n",
    "        print(f'{viterbi_start - model_start:.6f} / {viterbi_end - single_loop_start:.6f} = ' +\n",
    "             f'{(viterbi_start - model_start) / (viterbi_end - single_loop_start) * 100:5.2f}%')\n",
    "        \n",
    "        print(alignments[0].shape)\n",
    "        \n",
    "        break\n",
    "        \n",
    "    break\n",
    "       \n",
    "#         for j, (l, t) in enumerate(zip(text_lengths, mel_lengths)):\n",
    "#             alignments[j] = alignments[j][0, :l.item(), :t.item()].sum(dim=-1)\n",
    "#             np.save(f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_list[j]}_alignment.npy',\n",
    "#                     alignments[j].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
