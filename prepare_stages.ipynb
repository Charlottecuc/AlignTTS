{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this code twice after the stage0 and stage0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import hparams\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.model import Model\n",
    "from text import text_to_sequence, sequence_to_text\n",
    "from denoiser import Denoiser\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import librosa\n",
    "from modules.loss import MDNLoss\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "data_type = 'phone'\n",
    "checkpoint_path = f\"training_log/aligntts/stage0/checkpoint_40000\"\n",
    "state_dict = {}\n",
    "for k, v in torch.load(checkpoint_path)['state_dict'].items():\n",
    "    state_dict[k[7:]]=v\n",
    "\n",
    "\n",
    "model = Model(hparams).cuda()\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.cuda().eval()\n",
    "criterion = MDNLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017b0f96946b498cb4f639a6eada5544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=164.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77cdc505dbf478da574f1bb77e8b9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1915808695a441648952df60485fe6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['train', 'val', 'test']\n",
    "batch_size=64\n",
    "\n",
    "for dataset in datasets:\n",
    "    with open(f'filelists/ljs_audio_text_{dataset}_filelist.txt', 'r') as f:\n",
    "        lines_raw = [line.split('|') for line in f.read().splitlines()]\n",
    "        lines_list = [ lines_raw[batch_size*i:batch_size*(i+1)] \n",
    "                      for i in range(len(lines_raw)//batch_size+1)]\n",
    "        \n",
    "    for batch in tqdm(lines_list):\n",
    "        file_list, text_list, mel_list = [], [], []\n",
    "        text_lengths, mel_lengths=[], []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            file_name, _, text = batch[i]\n",
    "            file_list.append(file_name)\n",
    "            seq = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                               f'{data_type}_seq')\n",
    "            mel = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                               'melspectrogram')\n",
    "\n",
    "            seq = torch.from_numpy(np.load(f'{seq}/{file_name}_sequence.npy'))\n",
    "            mel = torch.from_numpy(np.load(f'{mel}/{file_name}_melspectrogram.npy'))\n",
    "            \n",
    "            text_list.append(seq)\n",
    "            mel_list.append(mel)\n",
    "            text_lengths.append(seq.size(0))\n",
    "            mel_lengths.append(mel.size(1))\n",
    "            \n",
    "        text_lengths = torch.LongTensor(text_lengths)\n",
    "        mel_lengths = torch.LongTensor(mel_lengths)\n",
    "        text_padded = torch.zeros(len(batch), text_lengths.max().item(), dtype=torch.long)\n",
    "        mel_padded = torch.zeros(len(batch), hparams.n_mel_channels, mel_lengths.max().item())\n",
    "        \n",
    "        for j in range(len(batch)):\n",
    "            text_padded[j, :text_list[j].size(0)] = text_list[j]\n",
    "            mel_padded[j, :, :mel_list[j].size(1)] = mel_list[j]\n",
    "        \n",
    "        text_padded = text_padded.cuda()\n",
    "        mel_padded = mel_padded.cuda()\n",
    "        text_lengths = text_lengths.cuda()\n",
    "        mel_lengths = mel_lengths.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_input = model.Prenet(text_padded)\n",
    "            hidden_states, _ = model.FFT_lower(encoder_input, text_lengths)\n",
    "            mu_sigma = model.get_mu_sigma(hidden_states)\n",
    "            _, log_prob_matrix = criterion(mu_sigma, mel_padded, text_lengths, mel_lengths)\n",
    "\n",
    "            align = model.viterbi(log_prob_matrix, text_lengths, mel_lengths).to(torch.long)\n",
    "            alignments = list(torch.split(align,1))\n",
    "       \n",
    "        for j, (l, t) in enumerate(zip(text_lengths, mel_lengths)):\n",
    "            alignments[j] = alignments[j][0, :l.item(), :t.item()].sum(dim=-1)\n",
    "            np.save(f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_list[j]}_alignment.npy',\n",
    "                    alignments[j].detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
