{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4928, -0.6266,  1.6426,  1.0207],\n",
       "        [-0.6854, -0.9873,  1.0441, -0.8110],\n",
       "        [-0.3582,  0.2052, -0.0492, -0.9293]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.randn(3,4)\n",
    "a[torch.arange(2), :torch.ra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import hparams\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.model import Model\n",
    "from text import text_to_sequence, sequence_to_text\n",
    "from denoiser import Denoiser\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import librosa\n",
    "from modules.loss import MDNLoss\n",
    "\n",
    "data_type = 'phone'\n",
    "checkpoint_path = f\"training_log/aligntts/checkpoint_40000\"\n",
    "state_dict = {}\n",
    "for k, v in torch.load(checkpoint_path)['state_dict'].items():\n",
    "    state_dict[k[7:]]=v\n",
    "\n",
    "\n",
    "model = Model(hparams).cuda()\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.cuda().eval()\n",
    "criterion = MDNLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e66ac4871b4b55b20abe19c18de313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6aa9cc4265046d8aeb6a83c863f06df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1368.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8798f4ab31604fc3b39a438539a5d62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1251.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "for dataset in datasets:\n",
    "    with open(f'filelists/ljs_audio_text_{dataset}_filelist.txt', 'r') as f:\n",
    "        lines = [line.split('|') for line in f.read().splitlines()]\n",
    "\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        file_name, _, text = lines[i]\n",
    "        text = '^' + text + '~'\n",
    "        seq = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                           f'{data_type}_seq')\n",
    "        mel = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                           'melspectrogram')\n",
    "\n",
    "        with open(f'{seq}/{file_name}_sequence.pkl', 'rb') as f:\n",
    "            text_padded = pkl.load(f).unsqueeze(0).cuda()\n",
    "        with open(f'{mel}/{file_name}_melspectrogram.pkl', 'rb') as f:\n",
    "            mel_padded = pkl.load(f).unsqueeze(0).cuda()\n",
    "        \n",
    "        mel_padded = (mel_padded - torch.min(mel_padded))\\\n",
    "                         / torch.max((mel_padded - torch.min(mel_padded)))\n",
    "        \n",
    "        text_lengths=torch.LongTensor([text_padded.size(1)]).cuda()\n",
    "        mel_lengths=torch.LongTensor([mel_padded.size(2)]).cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "        encoder_input = model.Prenet(text_padded)\n",
    "        hidden_states, _ = model.FFT_lower(encoder_input, text_lengths)\n",
    "        mu_sigma = model.get_mu_sigma(hidden_states)\n",
    "        _, log_prob_matrix = criterion(mu_sigma, mel_padded, text_lengths, mel_lengths)\n",
    "    \n",
    "        alignments = model.viterbi(log_prob_matrix[0:1], text_lengths[0:1], mel_lengths[0:1])[0].t()\n",
    "        \n",
    "        with open(f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_name}.pkl', 'wb') as f:\n",
    "            pkl.dump(alignments, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
