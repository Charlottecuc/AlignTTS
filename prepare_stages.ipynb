{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import hparams\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.model import Model\n",
    "from text import text_to_sequence, sequence_to_text\n",
    "from denoiser import Denoiser\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import librosa\n",
    "from modules.loss import MDNLoss\n",
    "\n",
    "# data_type = 'phone'\n",
    "# checkpoint_path = f\"training_log/aligntts/checkpoint_40000\"\n",
    "checkpoint_path = f\"training_log/aligntts/checkpoint_100000\"\n",
    "state_dict = {}\n",
    "\n",
    "for k, v in torch.load(checkpoint_path)['state_dict'].items():\n",
    "    state_dict[k[7:]]=v\n",
    "\n",
    "\n",
    "model = Model(hparams).cuda()\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.cuda().eval()\n",
    "criterion = MDNLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import decode_text, display_hparams\n",
    "\n",
    "display_hparams(hparams)\n",
    "\n",
    "data_type = hparams.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.utils import decode_text\n",
    "\n",
    "def save_alignment_diagram(alignments, text_padded, text_lengths, mel_padded, file_path='align.png'):\n",
    "    \n",
    "    decoded_text = decode_text(text_padded, text_lengths)\n",
    "\n",
    "    alignment_dict = dict()\n",
    "\n",
    "    for t, char_order in (alignments == 1).nonzero():\n",
    "        alignment_dict.setdefault(int(char_order), []).append(int(t))\n",
    "\n",
    "    xticks = []\n",
    "    x_redline = []\n",
    "\n",
    "    for key in alignment_dict.keys():\n",
    "\n",
    "        xticks.append(np.mean(alignment_dict[key]))\n",
    "        x_redline.append(alignment_dict[key][-1] + .5)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(60, 16), sharex=True)\n",
    "    axes[0].imshow(alignments.cpu().numpy().T[:, :], aspect='auto', origin='reversed')\n",
    "    axes[1].imshow(mel_padded.cpu().numpy()[0, :, :], aspect='auto', origin='reversed')\n",
    "\n",
    "    plt.xticks(xticks, text, fontsize=24)\n",
    "    axes[0].set_yticks([])\n",
    "    axes[1].set_yticks([])\n",
    "\n",
    "    for x in x_redline:\n",
    "        axes[0].axvline(x=x, color='white')\n",
    "        axes[1].axvline(x=x, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_existing = True\n",
    "\n",
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "for dataset in datasets:\n",
    "    with open(f'filelists/ljs_audio_text_{dataset}_filelist.txt', 'r') as f:\n",
    "        lines = [line.split('|') for line in f.read().splitlines()]\n",
    "\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        file_name, _, text = lines[i]\n",
    "        \n",
    "        save_name = f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_name}.pkl'\n",
    "        \n",
    "        if os.path.isfile(save_name) and skip_existing:\n",
    "            continue\n",
    "        \n",
    "        text = '^' + text + '~'\n",
    "#         print(text)\n",
    "        seq = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                           f'{data_type}')\n",
    "        mel = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                           'melspectrogram')\n",
    "\n",
    "        with open(f'{seq}/{file_name}_sequence.pkl', 'rb') as f:\n",
    "            text_padded = pkl.load(f).unsqueeze(0).cuda()\n",
    "        with open(f'{mel}/{file_name}_melspectrogram.pkl', 'rb') as f:\n",
    "            mel_padded = pkl.load(f).unsqueeze(0).cuda()\n",
    "        \n",
    "        mel_padded = (mel_padded - torch.min(mel_padded))\\\n",
    "                         / torch.max((mel_padded - torch.min(mel_padded)))\n",
    "        \n",
    "        text_lengths=torch.LongTensor([text_padded.size(1)]).cuda()\n",
    "        mel_lengths=torch.LongTensor([mel_padded.size(2)]).cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "        encoder_input = model.Prenet(text_padded)\n",
    "        hidden_states, _ = model.FFT_lower(encoder_input, text_lengths)\n",
    "        mu_sigma = model.get_mu_sigma(hidden_states)\n",
    "        _, log_prob_matrix = criterion(mu_sigma, mel_padded, text_lengths, mel_lengths)\n",
    "    \n",
    "        alignments = model.viterbi(log_prob_matrix[0:1], text_lengths[0:1], mel_lengths[0:1])[0].t()\n",
    "        \n",
    "        with open(f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_name}.pkl', 'wb') as f:\n",
    "            pkl.dump(alignments, f)\n",
    "            \n",
    "        save_alignment_diagram(alignments, text_padded, text_lengths, mel_padded, \n",
    "                               f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_name}.png')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
