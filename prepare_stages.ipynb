{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import hparams\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.model import Model\n",
    "from text import text_to_sequence, sequence_to_text\n",
    "from denoiser import Denoiser\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import librosa\n",
    "from modules.loss import MDNLoss\n",
    "\n",
    "# data_type = 'phone'\n",
    "# checkpoint_path = f\"training_log/aligntts/checkpoint_40000\"\n",
    "checkpoint_path = f\"training_log/aligntts/checkpoint_100000\"\n",
    "state_dict = {}\n",
    "\n",
    "for k, v in torch.load(checkpoint_path)['state_dict'].items():\n",
    "    state_dict[k[7:]]=v\n",
    "\n",
    "\n",
    "model = Model(hparams).cuda()\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.cuda().eval()\n",
    "criterion = MDNLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        accumulation: 4\n",
      "          batch_size: 8\n",
      "           data_path: ../Dataset/LJSpeech-1.1/preprocessed\n",
      "           data_type: char_seq\n",
      "         dprenet_dim: 256\n",
      "              ff_dim: 1024\n",
      "       filter_length: 1024\n",
      "    grad_clip_thresh: 1.0\n",
      "          hidden_dim: 256\n",
      "          hop_length: 256\n",
      "iters_per_checkpoint: 10000\n",
      "iters_per_validation: 4000\n",
      "       log_directory: aligntts\n",
      "                  lr: 0.05103103630798288\n",
      "            mel_fmax: 8000.0\n",
      "            mel_fmin: 0\n",
      "              n_gpus: 2\n",
      "             n_heads: 2\n",
      "            n_layers: 6\n",
      "      n_mel_channels: 80\n",
      "           n_symbols: 119\n",
      "    output_directory: training_log\n",
      "         postnet_dim: 256\n",
      "       sampling_rate: 22050\n",
      "                seed: 1234\n",
      "             symbols: ['_', '^', '~', ' ', ',', '.', \"'\", '?', '!', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '@AA', '@AA0', '@AA1', '@AA2', '@AE', '@AE0', '@AE1', '@AE2', '@AH', '@AH0', '@AH1', '@AH2', '@AO', '@AO0', '@AO1', '@AO2', '@AW', '@AW0', '@AW1', '@AW2', '@AY', '@AY0', '@AY1', '@AY2', '@B', '@CH', '@D', '@DH', '@EH', '@EH0', '@EH1', '@EH2', '@ER', '@ER0', '@ER1', '@ER2', '@EY', '@EY0', '@EY1', '@EY2', '@F', '@G', '@HH', '@IH', '@IH0', '@IH1', '@IH2', '@IY', '@IY0', '@IY1', '@IY2', '@JH', '@K', '@L', '@M', '@N', '@NG', '@OW', '@OW0', '@OW1', '@OW2', '@OY', '@OY0', '@OY1', '@OY2', '@P', '@R', '@S', '@SH', '@T', '@TH', '@UH', '@UH0', '@UH1', '@UH2', '@UW', '@UW0', '@UW1', '@UW2', '@V', '@W', '@Y', '@Z', '@ZH']\n",
      "symbols_embedding_dim: 256\n",
      "       text_cleaners: ['english_cleaners']\n",
      "         train_steps: [40000, 40000, 80000, 10000]\n",
      "      training_files: filelists/ljs_audio_text_train_filelist.txt\n",
      "    validation_files: filelists/ljs_audio_text_val_filelist.txt\n",
      "        warmup_steps: 4000\n",
      "          win_length: 1024\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import decode_text, display_hparams\n",
    "\n",
    "display_hparams(hparams)\n",
    "\n",
    "data_type = hparams.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.utils import decode_text\n",
    "\n",
    "def save_alignment_diagram(alignments, text_padded, text_lengths, mel_padded, file_path='align.png'):\n",
    "    \n",
    "    decoded_text = decode_text(text_padded, text_lengths)\n",
    "\n",
    "    alignment_dict = dict()\n",
    "\n",
    "    for t, char_order in (alignments == 1).nonzero():\n",
    "        alignment_dict.setdefault(int(char_order), []).append(int(t))\n",
    "\n",
    "    xticks = []\n",
    "    x_redline = []\n",
    "\n",
    "    for key in alignment_dict.keys():\n",
    "\n",
    "        xticks.append(np.mean(alignment_dict[key]))\n",
    "        x_redline.append(alignment_dict[key][-1] + .5)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(60, 16), sharex=True)\n",
    "    axes[0].imshow(alignments.cpu().numpy().T[:, :], aspect='auto', origin='reversed')\n",
    "    axes[1].imshow(mel_padded.cpu().numpy()[0, :, :], aspect='auto', origin='reversed')\n",
    "\n",
    "    plt.xticks(xticks, text, fontsize=24)\n",
    "    axes[0].set_yticks([])\n",
    "    axes[1].set_yticks([])\n",
    "\n",
    "    for x in x_redline:\n",
    "        axes[0].axvline(x=x, color='white')\n",
    "        axes[1].axvline(x=x, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00675833014542319f48970e2bd4ff57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "for dataset in datasets:\n",
    "    with open(f'filelists/ljs_audio_text_{dataset}_filelist.txt', 'r') as f:\n",
    "        lines = [line.split('|') for line in f.read().splitlines()]\n",
    "\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        file_name, _, text = lines[i]\n",
    "        text = '^' + text + '~'\n",
    "#         print(text)\n",
    "        seq = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                           f'{data_type}')\n",
    "        mel = os.path.join('../Dataset/LJSpeech-1.1/preprocessed',\n",
    "                           'melspectrogram')\n",
    "\n",
    "        with open(f'{seq}/{file_name}_sequence.pkl', 'rb') as f:\n",
    "            text_padded = pkl.load(f).unsqueeze(0).cuda()\n",
    "        with open(f'{mel}/{file_name}_melspectrogram.pkl', 'rb') as f:\n",
    "            mel_padded = pkl.load(f).unsqueeze(0).cuda()\n",
    "        \n",
    "        mel_padded = (mel_padded - torch.min(mel_padded))\\\n",
    "                         / torch.max((mel_padded - torch.min(mel_padded)))\n",
    "        \n",
    "        text_lengths=torch.LongTensor([text_padded.size(1)]).cuda()\n",
    "        mel_lengths=torch.LongTensor([mel_padded.size(2)]).cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "        encoder_input = model.Prenet(text_padded)\n",
    "        hidden_states, _ = model.FFT_lower(encoder_input, text_lengths)\n",
    "        mu_sigma = model.get_mu_sigma(hidden_states)\n",
    "        _, log_prob_matrix = criterion(mu_sigma, mel_padded, text_lengths, mel_lengths)\n",
    "    \n",
    "        alignments = model.viterbi(log_prob_matrix[0:1], text_lengths[0:1], mel_lengths[0:1])[0].t()\n",
    "        \n",
    "        with open(f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_name}.pkl', 'wb') as f:\n",
    "            pkl.dump(alignments, f)\n",
    "            \n",
    "        save_alignment_diagram(alignments, text_padded, text_lengths, mel_padded, \n",
    "                               f'../Dataset/LJSpeech-1.1/preprocessed/alignments/{file_name}.png')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
